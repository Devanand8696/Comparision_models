{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"nvo6FaKTLs2p","outputId":"4d1a2fad-a7f6-4b8d-cb29-bb8ec811ccf3","executionInfo":{"status":"error","timestamp":1696435894076,"user_tz":-330,"elapsed":33884,"user":{"displayName":"Devanand C M","userId":"10111919713246348327"}}},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["%run /content/drive/MyDrive/LLFormer-main-g/LLFormer-main/warmup_scheduler/scheduler.py"],"metadata":{"id":"IFQfEzGtNS_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.optim.lr_scheduler import StepLR, ExponentialLR\n","from torch.optim.sgd import SGD\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\n","    optim = SGD(model, 0.1)\n","\n","    # scheduler_warmup is chained with schduler_steplr\n","    scheduler_steplr = StepLR(optim, step_size=10, gamma=0.1)\n","    scheduler_warmup = GradualWarmupScheduler(optim, multiplier=1, total_epoch=5, after_scheduler=scheduler_steplr)\n","\n","    # this zero gradient update is needed to avoid a warning message, issue #8.\n","    optim.zero_grad()\n","    optim.step()\n","\n","    for epoch in range(1, 20):\n","        scheduler_warmup.step(epoch)\n","        print(epoch, optim.param_groups[0]['lr'])\n","\n","        optim.step()    # backward pass (update network)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1abPLbfNx64","outputId":"7b105361-aa51-4f4b-d9b9-011934e8a20b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 0.020000000000000004\n","2 0.04000000000000001\n","3 0.06\n","4 0.08000000000000002\n","5 0.1\n","6 0.1\n","7 0.1\n","8 0.1\n","9 0.1\n","10 0.1\n","11 0.1\n","12 0.1\n","13 0.1\n","14 0.1\n","15 0.010000000000000002\n","16 0.010000000000000002\n","17 0.010000000000000002\n","18 0.010000000000000002\n","19 0.010000000000000002\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"]}]},{"cell_type":"code","source":["!pip install pytorch-msssim\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7j8EVcbN_yh","outputId":"df8a7095-0d9c-4461-c401-23422bc01056"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-msssim\n","  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-msssim) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-msssim) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-msssim) (17.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-msssim) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-msssim) (1.3.0)\n","Installing collected packages: pytorch-msssim\n","Successfully installed pytorch-msssim-1.0.0\n"]}]},{"cell_type":"code","source":["!pip install matplotlib scikit-image opencv-python yacs joblib natsort h5py tqdm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcXPxla0PnFc","outputId":"3e6fa61d-35a8-4ec6-d4f1-a04055d4b095"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.4)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.9.26)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Installing collected packages: yacs\n","Successfully installed yacs-0.1.8\n"]}]},{"cell_type":"code","source":["!pip install tensorboardX\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvPVmZNvPj9P","outputId":"7cdc2884-86d4-490a-ded6-101cf1edbd67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m81.9/101.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}]},{"cell_type":"code","source":["!pip install einops\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nywXQSVoP-uF","outputId":"68573a8d-dd1f-4d2a-c5ca-498796a9317b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}]},{"cell_type":"code","source":["%run /content/drive/MyDrive/LLFormer-main-g/LLFormer-main/train.py  -yml_path /content/drive/MyDrive/dataset/training_LOL.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"id":"od9iA1gzQCGp","outputId":"d294f150-ea51-4fd0-cf6a-ed25ace634aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["load training yaml file: /content/drive/MyDrive/dataset/training_LOL.yaml\n","==> Build the model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"]},{"output_type":"stream","name":"stdout","text":["==> Loading datasets\n","Input Image Files:\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/LLFormer-main-g/LLFormer-main/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Loading datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'patch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN_PS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m train_loader = DataLoader(dataset=train_dataset, batch_size=OPT['BATCH'],\n\u001b[0m\u001b[1;32m    104\u001b[0m                           shuffle=True, num_workers=8, drop_last=False)\n\u001b[1;32m    105\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_validation_data2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'patch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VAL_PS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import cv2\n","from pytorch_msssim import ssim\n","import math\n","\n","def calculate_psnr(img1, img2, border=0):\n","    # img1 and img2 have range [0, 255]\n","    #img1 = img1.squeeze()\n","    #img2 = img2.squeeze()\n","    if not img1.shape == img2.shape:\n","        raise ValueError('Input images must have the same dimensions.')\n","    h, w = img1.shape[:2]\n","    img1 = img1[border:h-border, border:w-border]\n","    img2 = img2[border:h-border, border:w-border]\n","\n","    img1 = img1.astype(np.float64)\n","    img2 = img2.astype(np.float64)\n","    mse = np.mean((img1 - img2)**2)\n","    if mse == 0:\n","        return float('inf')\n","    return 20 * math.log10(255.0 / math.sqrt(mse))\n","\n","\n","# --------------------------------------------\n","# SSIM\n","# --------------------------------------------\n","def calculate_ssim(img1, img2, border=0):\n","    '''calculate SSIM\n","    the same outputs as MATLAB's\n","    img1, img2: [0, 255]\n","    '''\n","    #img1 = img1.squeeze()\n","    #img2 = img2.squeeze()\n","    if not img1.shape == img2.shape:\n","        raise ValueError('Input images must have the same dimensions.')\n","    h, w = img1.shape[:2]\n","    img1 = img1[border:h-border, border:w-border]\n","    img2 = img2[border:h-border, border:w-border]\n","\n","    if img1.ndim == 2:\n","        return ssim(img1, img2)\n","    elif img1.ndim == 3:\n","        if img1.shape[2] == 3:\n","            ssims = []\n","            for i in range(3):\n","                ssims.append(ssim(img1[:,:,i], img2[:,:,i]))\n","            return np.array(ssims).mean()\n","        elif img1.shape[2] == 1:\n","            return ssim(np.squeeze(img1), np.squeeze(img2))\n","    else:\n","        raise ValueError('Wrong input image dimensions.')\n","\n","def load_img(filepath):\n","    return cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n","\n","def torchPSNR(tar_img, prd_img):\n","    imdff = torch.clamp(prd_img, 0, 1) - torch.clamp(tar_img, 0, 1)\n","    rmse = (imdff**2).mean().sqrt()\n","    ps = 20*torch.log10(1/rmse)\n","    return ps\n","\n","def batch_PSNR(img1, img2, data_range=None):\n","    PSNR = []\n","    for im1, im2 in zip(img1, img2):\n","        psnr = torchPSNR(im1, im2)\n","        PSNR.append(psnr)\n","    return sum(PSNR)/len(PSNR)\n","\n","def torchSSIM(tar_img, prd_img):\n","    return ssim(tar_img, prd_img, data_range=1.0, size_average=True)\n","\n","def save_img(filepath, img):\n","    cv2.imwrite(filepath, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n","\n","def numpyPSNR(tar_img, prd_img):\n","    imdff = np.float32(prd_img) - np.float32(tar_img)\n","    rmse = np.sqrt(np.mean(imdff**2))\n","    ps = 20*np.log10(255/rmse)\n","    return ps\n","import torch\n","\n","\"\"\"\n","The following reference from:\n","https://github.com/oblime/RGB_HSV_HSL\n","\"\"\"\n","def rgb2hsl_torch(rgb: torch.Tensor) -> torch.Tensor:\n","    cmax, cmax_idx = torch.max(rgb, dim=1, keepdim=True)\n","    cmin = torch.min(rgb, dim=1, keepdim=True)[0]\n","    delta = cmax - cmin\n","    hsl_h = torch.empty_like(rgb[:, 0:1, :, :])\n","    cmax_idx[delta == 0] = 3\n","    hsl_h[cmax_idx == 0] = (((rgb[:, 1:2] - rgb[:, 2:3]) / delta) % 6)[cmax_idx == 0]\n","    hsl_h[cmax_idx == 1] = (((rgb[:, 2:3] - rgb[:, 0:1]) / delta) + 2)[cmax_idx == 1]\n","    hsl_h[cmax_idx == 2] = (((rgb[:, 0:1] - rgb[:, 1:2]) / delta) + 4)[cmax_idx == 2]\n","    hsl_h[cmax_idx == 3] = 0.\n","    hsl_h /= 6.\n","\n","    hsl_l = (cmax + cmin) / 2.\n","    hsl_s = torch.empty_like(hsl_h)\n","    hsl_s[hsl_l == 0] = 0\n","    hsl_s[hsl_l == 1] = 0\n","    hsl_l_ma = torch.bitwise_and(hsl_l > 0, hsl_l < 1)\n","    hsl_l_s0_5 = torch.bitwise_and(hsl_l_ma, hsl_l <= 0.5)\n","    hsl_l_l0_5 = torch.bitwise_and(hsl_l_ma, hsl_l > 0.5)\n","    hsl_s[hsl_l_s0_5] = ((cmax - cmin) / (hsl_l * 2.))[hsl_l_s0_5]\n","    hsl_s[hsl_l_l0_5] = ((cmax - cmin) / (- hsl_l * 2. + 2.))[hsl_l_l0_5]\n","    return torch.cat([hsl_h, hsl_s, hsl_l], dim=1)\n","\n","\n","def rgb2hsv_torch(rgb: torch.Tensor) -> torch.Tensor:\n","    cmax, cmax_idx = torch.max(rgb, dim=1, keepdim=True)\n","    cmin = torch.min(rgb, dim=1, keepdim=True)[0]\n","    delta = cmax - cmin\n","    hsv_h = torch.empty_like(rgb[:, 0:1, :, :])\n","    cmax_idx[delta == 0] = 3\n","    hsv_h[cmax_idx == 0] = (((rgb[:, 1:2] - rgb[:, 2:3]) / delta) % 6)[cmax_idx == 0]\n","    hsv_h[cmax_idx == 1] = (((rgb[:, 2:3] - rgb[:, 0:1]) / delta) + 2)[cmax_idx == 1]\n","    hsv_h[cmax_idx == 2] = (((rgb[:, 0:1] - rgb[:, 1:2]) / delta) + 4)[cmax_idx == 2]\n","    hsv_h[cmax_idx == 3] = 0.\n","    hsv_h /= 6.\n","    hsv_s = torch.where(cmax == 0, torch.tensor(0.).type_as(rgb), delta / cmax)\n","    hsv_v = cmax\n","    return torch.cat([hsv_h, hsv_s, hsv_v], dim=1)\n","\n","\n","def hsv2rgb_torch(hsv: torch.Tensor) -> torch.Tensor:\n","    hsv_h, hsv_s, hsv_l = hsv[:, 0:1], hsv[:, 1:2], hsv[:, 2:3]\n","    _c = hsv_l * hsv_s\n","    _x = _c * (- torch.abs(hsv_h * 6. % 2. - 1) + 1.)\n","    _m = hsv_l - _c\n","    _o = torch.zeros_like(_c)\n","    idx = (hsv_h * 6.).type(torch.uint8)\n","    idx = (idx % 6).expand(-1, 3, -1, -1)\n","    rgb = torch.empty_like(hsv)\n","    rgb[idx == 0] = torch.cat([_c, _x, _o], dim=1)[idx == 0]\n","    rgb[idx == 1] = torch.cat([_x, _c, _o], dim=1)[idx == 1]\n","    rgb[idx == 2] = torch.cat([_o, _c, _x], dim=1)[idx == 2]\n","    rgb[idx == 3] = torch.cat([_o, _x, _c], dim=1)[idx == 3]\n","    rgb[idx == 4] = torch.cat([_x, _o, _c], dim=1)[idx == 4]\n","    rgb[idx == 5] = torch.cat([_c, _o, _x], dim=1)[idx == 5]\n","    rgb += _m\n","    return rgb\n","\n","\n","def hsl2rgb_torch(hsl: torch.Tensor) -> torch.Tensor:\n","    hsl_h, hsl_s, hsl_l = hsl[:, 0:1], hsl[:, 1:2], hsl[:, 2:3]\n","    _c = (-torch.abs(hsl_l * 2. - 1.) + 1) * hsl_s\n","    _x = _c * (-torch.abs(hsl_h * 6. % 2. - 1) + 1.)\n","    _m = hsl_l - _c / 2.\n","    idx = (hsl_h * 6.).type(torch.uint8)\n","    idx = (idx % 6).expand(-1, 3, -1, -1)\n","    rgb = torch.empty_like(hsl)\n","    _o = torch.zeros_like(_c)\n","    rgb[idx == 0] = torch.cat([_c, _x, _o], dim=1)[idx == 0]\n","    rgb[idx == 1] = torch.cat([_x, _c, _o], dim=1)[idx == 1]\n","    rgb[idx == 2] = torch.cat([_o, _c, _x], dim=1)[idx == 2]\n","    rgb[idx == 3] = torch.cat([_o, _x, _c], dim=1)[idx == 3]\n","    rgb[idx == 4] = torch.cat([_x, _o, _c], dim=1)[idx == 4]\n","    rgb[idx == 5] = torch.cat([_c, _o, _x], dim=1)[idx == 5]\n","    rgb += _m\n","    return rgb\n"],"metadata":{"id":"qaqrGr2e9z6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from torch.utils.data import Dataset\n","import torch\n","from PIL import Image\n","import torchvision.transforms.functional as TF\n","import random\n","import numpy as np\n","#from utils.image_utils import load_img\n","\n","#import torch.nn.functional as F\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])\n","\n","\n","class DataLoaderTrain(Dataset):\n","    def __init__(self, rgb_dir, img_options=None):\n","        super(DataLoaderTrain, self).__init__()\n"," #       print(1)\n","        inp_files =\"/content/drive/MyDrive/dataset/LOLdataset_main/our485/low_sub\"\n","        tar_files =\"/content/drive/MyDrive/dataset/LOLdataset_main/our485/high_sub\"\n","        self.inp_filenames = [os.path.join(inp_files, x) for x in os.listdir( inp_files) if is_image_file( x)]\n","        self.tar_filenames = [os.path.join(tar_files, x) for x in os.listdir(tar_files) if is_image_file(x)]\n","    #    self.inp_filenames = [os.path.join(inp_files, x) for x in inp_files]\n","    #    self.tar_filenames = [os.path.join(tar_files, x) for x in tar_files ]\n","        from PIL import Image\n","        inp_count = len(self.inp_filenames)\n","        tar_count = len(self.tar_filenames)\n","     #   print(\"Input Directory Path:\", inp_files)\n","     #   self.inp_filenames = [os.path.join(inp_files, x) for x in inp_files ]\n","      #  print(\"Target Directory Path:\", tar_files)\n","       # self.tar_filenames = [os.path.join(tar_files, x) for x in tar_files ]\n","\n","   #     print(\"Number of input files:\", inp_count)\n","    #    print(\"Number of target files:\", tar_count)\n","\n","    #    print(\"Input Image Files:\")\n","    #    image_count = 0\n","     #   for x in os.listdir(inp_files):\n","      #        if is_image_file(x):\n","          #           print(x)\n","           #          if image_count < 2:\n","             #             img = Image.open(os.path.join(inp_files, x))\n","              #            img.show()\n","                 #         image_count += 1\n","\n","        self.img_options = img_options\n","        self.sizex = len(self.inp_filenames)  # get the size of target\n","\n","        self.ps = self.img_options['patch_size']\n","\n","    def __len__(self):\n","        return self.sizex\n","\n","    def __getitem__(self, index):\n","        index_ = index % self.sizex\n","      #  print(index_)\n","        ps = self.ps\n","      #  print(\"index:\", index)\n","      #  print(\"size:\",self.sizex)\n","      #  print(\"index_:\", index_)\n","       # print(\"len(inp_filenames):\", len(self.inp_filenames))\n","      #  print(\"len(tar_filenames):\", len(self.tar_filenames))\n","        inp_path = self.inp_filenames[index_]\n","        tar_path = self.tar_filenames[index_]\n","     #   print(13)\n","        inp_img = Image.open(inp_path).convert('RGB')\n","        tar_img = Image.open(tar_path).convert('RGB')\n","\n","        w, h = tar_img.size\n","        padw = ps - w if w < ps else 0\n","        padh = ps - h if h < ps else 0\n","      #  print(203)\n","        # Reflect Pad in case image is smaller than patch_size\n","        if padw != 0 or padh != 0:\n","            inp_img = TF.pad(inp_img, (0, 0, padw, padh), padding_mode='reflect')\n","            tar_img = TF.pad(tar_img, (0, 0, padw, padh), padding_mode='reflect')\n","\n","        inp_img = TF.to_tensor(inp_img)\n","        tar_img = TF.to_tensor(tar_img)\n","\n","        hh, ww = tar_img.shape[1], tar_img.shape[2]\n","\n","        rr = random.randint(0, hh - ps)\n","        cc = random.randint(0, ww - ps)\n","        aug = random.randint(1, 8)\n","\n","        # Crop patch\n","        inp_img = inp_img[:, rr:rr + ps, cc:cc + ps]\n","        tar_img = tar_img[:, rr:rr + ps, cc:cc + ps]\n","     #   print(23)\n","        # Data Augmentations\n","        if aug == 1:\n","            inp_img = inp_img.flip(1)\n","            tar_img = tar_img.flip(1)\n","        elif aug == 2:\n","            inp_img = inp_img.flip(2)\n","            tar_img = tar_img.flip(2)\n","        elif aug == 3:\n","            inp_img = torch.rot90(inp_img, dims=(1, 2))\n","            tar_img = torch.rot90(tar_img, dims=(1, 2))\n","        elif aug == 4:\n","            inp_img = torch.rot90(inp_img, dims=(1, 2), k=2)\n","            tar_img = torch.rot90(tar_img, dims=(1, 2), k=2)\n","        elif aug == 5:\n","            inp_img = torch.rot90(inp_img, dims=(1, 2), k=3)\n","            tar_img = torch.rot90(tar_img, dims=(1, 2), k=3)\n","        elif aug == 6:\n","            inp_img = torch.rot90(inp_img.flip(1), dims=(1, 2))\n","            tar_img = torch.rot90(tar_img.flip(1), dims=(1, 2))\n","        elif aug == 7:\n","            inp_img = torch.rot90(inp_img.flip(2), dims=(1, 2))\n","            tar_img = torch.rot90(tar_img.flip(2), dims=(1, 2))\n","\n","        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n","       # print(33)\n","        return tar_img, inp_img, filename\n","\n","\n","class DataLoaderVal(Dataset):\n","    def __init__(self, rgb_dir, img_options=None, rgb_dir2=None):\n","        super(DataLoaderVal, self).__init__()\n","\n","        inp_files ='/content/drive/MyDrive/dataset/LOLdataset_main/eval15/low_sub'\n","        tar_files ='/content/drive/MyDrive/dataset/LOLdataset_main/eval15/high_sub'\n","        self.inp_filenames = [os.path.join(inp_files, x) for x in os.listdir( inp_files) if is_image_file(x)]\n","        self.tar_filenames = [os.path.join(tar_files, x) for x in os.listdir( tar_files) if is_image_file(x)]\n","        print(2)\n","\n","        self.img_options = img_options\n","        self.sizex = len(self.tar_filenames)  # get the size of target\n","\n","        self.ps = self.img_options['patch_size']\n","\n","    def __len__(self):\n","        return self.sizex\n","\n","    def __getitem__(self, index):\n","        index_ = index % self.sizex\n","        ps = self.ps\n","\n","        inp_path = self.inp_filenames[index_]\n","        tar_path = self.tar_filenames[index_]\n","\n","        inp_img = Image.open(inp_path).convert('RGB')\n","        tar_img = Image.open(tar_path).convert('RGB')\n","\n","        # Validate on center crop\n","        if self.ps is not None:\n","            inp_img = TF.center_crop(inp_img, (ps, ps))\n","            tar_img = TF.center_crop(tar_img, (ps, ps))\n","\n","        inp_img = TF.to_tensor(inp_img)\n","        tar_img = TF.to_tensor(tar_img)\n","\n","        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n","\n","        return tar_img, inp_img, filename\n","\n","\n","class DataLoaderVal_(Dataset):\n","    def __init__(self, rgb_dir, img_options=None, rgb_dir2=None):\n","        super(DataLoaderVal_, self).__init__()\n","\n","        inp_files ='/content/drive/MyDrive/dataset/LOLdataset_main/eval15/low_sub'\n","        tar_files ='/content/drive/MyDrive/dataset/LOLdataset_main/eval15/high_sub'\n","        self.inp_filenames = [os.path.join(inp_files, x) for x in os.listdir( inp_files) if is_image_file(x)]\n","        self.tar_filenames = [os.path.join(tar_files, x) for x in os.listdir( tar_files) if is_image_file(x)]\n","\n","        self.img_options = img_options\n","        self.sizex = len(self.tar_filenames)  # get the size of target\n","        self.mul = 16\n","\n","    def __len__(self):\n","        return self.sizex\n","\n","    def __getitem__(self, index):\n","        index_ = index % self.sizex\n","\n","        inp_path = self.inp_filenames[index_]\n","        tar_path = self.tar_filenames[index_]\n","\n","        inp_img = Image.open(inp_path).convert('RGB')\n","        tar_img = Image.open(tar_path).convert('RGB')\n","        #inp_img = TF.to_tensor(inp_img)\n","        #tar_img = TF.to_tensor(tar_img)\n","        w, h = inp_img.size\n","        #h, w = inp_img.shape[2], inp_img.shape[3]\n","        H, W = ((h + self.mul) // self.mul) * self.mul, ((w + self.mul) // self.mul) * self.mul\n","        padh = H - h if h % self.mul != 0 else 0\n","        padw = W - w if w % self.mul != 0 else 0\n","        inp_img = TF.pad(inp_img, (0, 0, padw, padh), padding_mode='reflect')\n","        inp_img = TF.to_tensor(inp_img)\n","        tar_img = TF.to_tensor(tar_img)\n","        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n","\n","        return tar_img, inp_img, filename\n","\n","\n","class DataLoaderTest(Dataset):\n","    def __init__(self, inp_dir, img_options):\n","        super(DataLoaderTest, self).__init__()\n","\n","        inp_files = sorted(os.listdir(inp_dir))\n","        self.inp_filenames = [os.path.join(inp_dir, x) for x in os.listdir( inp_files) if is_image_file(x)]\n","\n","        self.inp_size = len(self.inp_filenames)\n","        self.img_options = img_options\n","\n","    def __len__(self):\n","        return self.inp_size\n","\n","    def __getitem__(self, index):\n","        path_inp = self.inp_filenames[index]\n","        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n","        inp = Image.open(path_inp).convert('RGB')\n","\n","        inp = TF.to_tensor(inp)\n","        return inp, filename\n","\n","\n","class DataLoaderTest_(Dataset):\n","    def __init__(self, rgb_dir, target_transform=None):\n","        super(DataLoaderTest_, self).__init__()\n","\n","        self.target_transform = target_transform\n","\n","\n","\n","        clean_files = '/content/drive/MyDrive/dataset/LOLdataset_main/test/low_sub'\n","        noisy_files ='/content/drive/MyDrive/dataset/LOLdataset_main/test/high_sub'\n","\n","        self.clean_filenames = [os.path.join(clean_files, x) for x in os.listdir( clean_files) if is_image_file(x)]\n","        self.noisy_filenames = [os.path.join(noisy_files, x) for x in os.listdir(  noisy_files) if is_image_file(x)]\n","\n","        self.tar_size = len(self.clean_filenames)\n","\n","    def __len__(self):\n","        return self.tar_size\n","\n","    def __getitem__(self, index):\n","        tar_index = index % self.tar_size\n","\n","        clean = torch.from_numpy(np.float32(load_img(self.clean_filenames[tar_index])))\n","        noisy = torch.from_numpy(np.float32(load_img(self.noisy_filenames[tar_index])))\n","\n","        clean_filename = os.path.split(self.clean_filenames[tar_index])[-1]\n","        noisy_filename = os.path.split(self.noisy_filenames[tar_index])[-1]\n","\n","        clean = clean.permute(2, 0, 1)\n","        noisy = noisy.permute(2, 0, 1)\n","\n","        return clean, noisy, clean_filename, noisy_filename\n"],"metadata":{"id":"NEcCKyMH-ibZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","#from transform.dataset_RGB import DataLoaderTrain, DataLoaderVal, DataLoaderTest_,DataLoaderVal_\n","\n","\n","def get_training_data(rgb_dir, img_options):\n","    assert os.path.exists(rgb_dir)\n","    return DataLoaderTrain(rgb_dir, img_options)\n","\n","\n","def get_validation_data(rgb_dir, img_options):\n","    assert os.path.exists(rgb_dir)\n","    return DataLoaderVal(rgb_dir, img_options)\n","\n","def get_validation_data2(rgb_dir, img_options):\n","    assert os.path.exists(rgb_dir)\n","    return DataLoaderVal_(rgb_dir, img_options)\n","\n","\n","def get_test_data(rgb_dir, img_options):\n","    assert os.path.exists(rgb_dir)\n","    return DataLoaderTest_(rgb_dir, img_options)\n"],"metadata":{"id":"Ipo-DvrG-r2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import yaml\n","\n","from utils import network_parameters, losses\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import time\n","import numpy as np\n","import random\n","#from transform.data_RGB import get_training_data,get_validation_data2\n","from warmup_scheduler import GradualWarmupScheduler\n","from tqdm import tqdm\n","from tensorboardX import SummaryWriter\n","import utils.losses\n","from model.LLFormer import LLFormer\n","import argparse\n","#parser = argparse.ArgumentParser(description='Hyper-parameters for LLFormer')\n","#parser.add_argument('-yml_path', default=\"./training.yaml\", type=str)\n","#args = parser.parse_args()"],"metadata":{"id":"fZGmVv6F-xRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Set Seeds\n","torch.backends.cudnn.benchmark = True\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.manual_seed(1234)\n","torch.cuda.manual_seed_all(1234)\n","\n","## Load yaml configuration file\n","yaml_file = '/content/drive/MyDrive/dataset/training_LOL.yaml'\n","\n","with open(yaml_file, 'r') as config:\n","    opt = yaml.safe_load(config)\n","print(\"load training yaml file: %s\"%(yaml_file))\n","\n","Train = opt['TRAINING']\n","OPT = opt['OPTIM']\n","\n","## Build Model\n","print('==> Build the model')\n","model_restored = LLFormer(inp_channels=3,out_channels=3,dim = 16,num_blocks = [2,4,8,16],num_refinement_blocks = 2,heads = [1,2,4,8],ffn_expansion_factor = 2.66,bias = False,LayerNorm_type = 'WithBias',attention=True,skip = False)\n","p_number = network_parameters(model_restored)\n","model_restored.cuda()\n","\n","## Training model path direction\n","mode = opt['MODEL']['MODE']\n","\n","model_dir = os.path.join(Train['SAVE_DIR'], mode, 'models')\n","utils.mkdir(model_dir)\n","train_dir = Train['TRAIN_DIR']\n","val_dir = Train['VAL_DIR']\n","\n","## GPU\n","gpus = ','.join([str(i) for i in opt['GPU']])\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n","device_ids = [i for i in range(torch.cuda.device_count())]\n","if torch.cuda.device_count() > 1:\n","    print(\"\\n\\nLet's use\", torch.cuda.device_count(), \"GPUs!\\n\\n\")\n","if len(device_ids) > 1:\n","    model_restored = nn.DataParallel(model_restored, device_ids=device_ids)\n","\n","## Optimizer\n","start_epoch = 1\n","new_lr = float(OPT['LR_INITIAL'])\n","optimizer = optim.Adam(model_restored.parameters(), lr=new_lr, betas=(0.9, 0.999), eps=1e-8)\n","\n","## Scheduler (Strategy)\n","warmup_epochs = 3\n","scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, OPT['EPOCHS'] - warmup_epochs,\n","                                                        eta_min=float(OPT['LR_MIN']))\n","scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n","scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBibvE3T_Ock","outputId":"7f27d47d-db6b-4d0a-ad7e-cc12a616a9b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["load training yaml file: /content/drive/MyDrive/dataset/training_LOL.yaml\n","==> Build the model\n"]}]},{"cell_type":"code","source":["# L1loss = nn.L1Loss()\n","Charloss = nn.SmoothL1Loss()\n","\n","## DataLoaders\n","print('==> Loading datasets')\n","train_dataset = get_training_data(train_dir, {'patch_size': Train['TRAIN_PS']})\n","train_loader = DataLoader(dataset=train_dataset, batch_size=OPT['BATCH'],\n","                          shuffle=True, num_workers=8, drop_last=False)\n","val_dataset = get_validation_data2(val_dir, {'patch_size': Train['VAL_PS']})\n","val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, num_workers=0,\n","                        drop_last=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taZMn61B_tlx","outputId":"8f84b190-5991-4e5e-8677-cd63c6a20dd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Loading datasets\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["#Show the training configuration\n","print(f'''==> Training details:\n","------------------------------------------------------------------\n","    Restoration mode:   {mode}\n","    Train patches size: {str(Train['TRAIN_PS']) + 'x' + str(Train['TRAIN_PS'])}\n","    Val patches size:   {str(Train['VAL_PS']) + 'x' + str(Train['VAL_PS'])}\n","    Model parameters:   {p_number}\n","    Start/End epochs:   {str(start_epoch) + '~' + str(OPT['EPOCHS'])}\n","    Batch sizes:        {OPT['BATCH']}\n","    Learning rate:      {OPT['LR_INITIAL']}\n","    GPU:                {'GPU' + str(device_ids)}''')\n","print('------------------------------------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPPJm6bzl_Bn","outputId":"6f95789b-7ce8-4042-84ee-8465273bc0fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Training details:\n","------------------------------------------------------------------\n","    Restoration mode:   LLFormer_LOL\n","    Train patches size: 128x128\n","    Val patches size:   128x128\n","    Model parameters:   24549014\n","    Start/End epochs:   1~1000\n","    Batch sizes:        8\n","    Learning rate:      1e-4\n","    GPU:                GPU[0]\n","------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["#Show the training configuration\n","print(f'''==> Training details:\n","------------------------------------------------------------------\n","    Restoration mode:   {mode}\n","    Train patches size: {str(Train['TRAIN_PS']) + 'x' + str(Train['TRAIN_PS'])}\n","    Val patches size:   {str(Train['VAL_PS']) + 'x' + str(Train['VAL_PS'])}\n","    Model parameters:   {p_number}\n","    Start/End epochs:   {str(start_epoch) + '~' + str(OPT['EPOCHS'])}\n","    Batch sizes:        {OPT['BATCH']}\n","    Learning rate:      {OPT['LR_INITIAL']}\n","    GPU:                {'GPU' + str(device_ids)}''')\n","print('------------------------------------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhfGpfGbhYbf","outputId":"d7eb65fb-a267-4b13-8240-42ed7f671461"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Training details:\n","------------------------------------------------------------------\n","    Restoration mode:   LLFormer_LOL\n","    Train patches size: 128x128\n","    Val patches size:   128x128\n","    Model parameters:   24549014\n","    Start/End epochs:   1~1000\n","    Batch sizes:        8\n","    Learning rate:      1e-4\n","    GPU:                GPU[0]\n","------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Start training!\n","print('==> Training start: ')\n","best_psnr = 0\n","best_ssim = 0\n","best_epoch_psnr = 0\n","best_epoch_ssim = 0\n","total_start_time = time.time()\n","\n","## Log\n","log_dir = os.path.join(Train['SAVE_DIR'], mode, 'log')\n","utils.mkdir(log_dir)\n","writer = SummaryWriter(log_dir=log_dir, filename_suffix=f'_{mode}')\n","\n","for epoch in range(start_epoch, OPT['EPOCHS'] + 1):\n","    epoch_start_time = time.time()\n","    epoch_loss = 0\n","    train_id = 1\n","\n","    model_restored.train()\n","    for i, data in enumerate(tqdm(train_loader), 0):\n","        # Forward propagation\n","        for param in model_restored.parameters():\n","            param.grad = None\n","        target = data[0].cuda()\n","        input_ = data[1].cuda()\n","        restored = model_restored(input_)\n","\n","        # Compute loss\n","        loss = Charloss(restored, target)\n","\n","        # Back propagation\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    ## Evaluation (Validation)\n","    if epoch % Train['VAL_AFTER_EVERY'] == 0:\n","        model_restored.eval()\n","        psnr_val_rgb = []\n","        ssim_val_rgb = []\n","        for ii, data_val in enumerate(val_loader, 0):\n","            target = data_val[0].cuda()\n","            input_ = data_val[1].cuda()\n","            h, w = target.shape[2], target.shape[3]\n","            with torch.no_grad():\n","                restored = model_restored(input_)\n","                restored = restored[:, :, :h, :w]\n","\n","            for res, tar in zip(restored, target):\n","                psnr_val_rgb.append(utils.torchPSNR(res, tar))\n","                ssim_val_rgb.append(utils.torchSSIM(restored, target))\n","\n","        psnr_val_rgb = torch.stack(psnr_val_rgb).mean().item()\n","        ssim_val_rgb = torch.stack(ssim_val_rgb).mean().item()\n","\n","        # Save the best PSNR model of validation\n","        if psnr_val_rgb > best_psnr:\n","            best_psnr = psnr_val_rgb\n","            best_epoch_psnr = epoch\n","            torch.save({'epoch': epoch,\n","                        'state_dict': model_restored.state_dict(),\n","                        'optimizer': optimizer.state_dict()\n","                        }, os.path.join(model_dir, \"model_bestPSNR.pth\"))\n","        print(\"[epoch %d PSNR: %.4f --- best_epoch %d Best_PSNR %.4f]\" % (\n","            epoch, psnr_val_rgb, best_epoch_psnr, best_psnr))\n","\n","        # Save the best SSIM model of validation\n","        if ssim_val_rgb > best_ssim:\n","            best_ssim = ssim_val_rgb\n","            best_epoch_ssim = epoch\n","            torch.save({'epoch': epoch,\n","                        'state_dict': model_restored.state_dict(),\n","                        'optimizer': optimizer.state_dict()\n","                        }, os.path.join(model_dir, \"model_bestSSIM.pth\"))\n","        print(\"[epoch %d SSIM: %.4f --- best_epoch %d Best_SSIM %.4f]\" % (\n","            epoch, ssim_val_rgb, best_epoch_ssim, best_ssim))\n","\n","        \"\"\"\n","        # Save evey epochs of model\n","        torch.save({'epoch': epoch,\n","                    'state_dict': model_restored.state_dict(),\n","                    'optimizer': optimizer.state_dict()\n","                    }, os.path.join(model_dir, f\"model_epoch_{epoch}.pth\"))\n","        \"\"\"\n","\n","        writer.add_scalar('val/PSNR', psnr_val_rgb, epoch)\n","        writer.add_scalar('val/SSIM', ssim_val_rgb, epoch)\n","    scheduler.step()\n","\n","    print(\"------------------------------------------------------------------\")\n","    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time() - epoch_start_time,\n","                                                                              epoch_loss, scheduler.get_lr()[0]))\n","    print(\"------------------------------------------------------------------\")\n","\n","    # Save the last model\n","    torch.save({'epoch': epoch,\n","                'state_dict': model_restored.state_dict(),\n","                'optimizer': optimizer.state_dict()\n","                }, os.path.join(model_dir, \"model_latest.pth\"))\n","\n","    writer.add_scalar('train/loss', epoch_loss, epoch)\n","    writer.add_scalar('train/lr', scheduler.get_lr()[0], epoch)\n","writer.close()\n","\n","total_finish_time = (time.time() - total_start_time)  # seconds\n","print('Total training time: {:.1f} hours'.format((total_finish_time / 60 / 60)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6JadpMQYmeru","outputId":"9a9063de-57ef-4cca-90e3-dc0f52f832c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Training start: \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["186\n","index: 16440186\n","455\n","\n","size:415\n","250index:index:38646 \n","\n"," \n","473 index:\n","\n","index:440index:455 index:index:index_:\n","  \n","386   46size:size:415\n","16250186\n"," \n"," size:\n","\n","473size:\n","473size:\n"," \n","size:   index_:size:len(inp_filenames):473473473  index_:473 \n","\n","473\n"," 455index_:\n","index_:473\n","index_:440  index_:\n"," \n","index_: 46\n","386len(tar_filenames): len(inp_filenames):16len(inp_filenames):\n","\n","250  len(inp_filenames):len(inp_filenames): \n","415483\n","473\n","len(inp_filenames):  473len(inp_filenames):\n","\n","len(inp_filenames):473473   \n","473\n","\n","473len(tar_filenames):473len(tar_filenames):len(tar_filenames):\n","len(tar_filenames):\n","\n","    len(tar_filenames):len(tar_filenames):len(tar_filenames):483483483 483\n","\n"," \n"," 483\n","483483\n","\n","\n","56\n","275index:360 56\n","\n","\n","size:index: index: 360 \n","473275size:\n","\n"," index_:3size:473 \n"," \n","56index:index_:473\n","  349\n","3175\n","index:\n","421 index:360\n","index_:len(inp_filenames):78349 \n","index:\n","\n","len(inp_filenames):175   \n","size:index:\n"," 275421  size:473473\n","size:\n","\n","78473 \n","len(inp_filenames):size:473\n","\n","len(tar_filenames):   len(tar_filenames): size:index_:473473\n"," 473 483\n","\n","483 index_:\n","\n","473len(tar_filenames):index_:349\n","\n","index_: \n","  index_:175 len(inp_filenames):421 483\n","3 78\n","473\n","len(inp_filenames):\n","\n","len(inp_filenames):len(inp_filenames):\n","len(inp_filenames): len(tar_filenames):  473 473 473\n","473\n","483\n","len(tar_filenames):\n","len(tar_filenames):len(tar_filenames):\n"," len(tar_filenames):   483483483483\n","\n","\n","\n","127197\n","\n","index:index:  127197\n","\n","size:size:  473473\n","\n","index_:index_:  127197\n","\n","len(inp_filenames):len(inp_filenames):379375  \n","473\n","index:473459\n"," \n","index:len(tar_filenames): len(tar_filenames):379\n"," 375 \n","index:483483size:\n","  \n","\n","459size:473285\n"," \n","size:\n","index_:473index:  \n","473 379index_:\n","285 \n","index_:\n"," size:len(inp_filenames):  473473\n","\n","len(tar_filenames):index_:459375  \n","\n","285483len(inp_filenames):len(inp_filenames):\n","\n","  len(inp_filenames):473473 \n","\n","473148len(tar_filenames):len(tar_filenames):\n"," 225 len(tar_filenames):\n","483 index:\n","483\n","483\n"," \n","index:148 \n","393size:225\n","\n","size:index:  473 \n","393index_:473\n"," size:\n","148 342139\n","index_:\n","index:473 464\n"," 342len(inp_filenames):\n","index:\n","225\n","\n"," 473size:index_: index: \n","393len(tar_filenames): \n"," 464len(inp_filenames):483\n"," \n","139473size:\n","len(inp_filenames):  473 \n","473\n","size:\n","index_:473len(tar_filenames): \n","len(tar_filenames):  342483473 \n","454\n","\n","len(inp_filenames):index_:483\n"," 48464\n","index_:  473\n","len(tar_filenames):\n"," index:\n","139index:len(inp_filenames):483\n","   \n","473len(inp_filenames):48454\n","\n","len(tar_filenames):\n","  473483size:size: \n","\n"," 47347392len(tar_filenames):\n","index_:\n","index_:\n","  91index:  4892\n","483\n","size: 473454\n","\n","\n","index:\n","len(inp_filenames):len(inp_filenames):index_:    92473473\n","\n","len(inp_filenames): \n","91473len(tar_filenames):25\n","\n"," len(tar_filenames):len(tar_filenames):\n","size:483  \n"," index:473483\n","\n","483index_:\n","  25\n","91size:\n"," len(inp_filenames):473 \n","2473\n","\n","index_:239 378len(tar_filenames):25index:\n","len(inp_filenames):\n","  \n","483473 \n","index:len(tar_filenames):\n","index:2  239\n","size:\n"," 483 size:378\n"," \n","473\n","473size: \n","473index_: \n","2index_:\n","len(inp_filenames): index_:239  \n","len(inp_filenames): 473378\n","len(inp_filenames): 473\n","473\n","\n","len(tar_filenames): 483\n","len(tar_filenames): len(tar_filenames):483 \n","347249483\n","\n","\n","index:index:232358 \n"," 249index:\n","\n","347 index:358\n","size:\n"," size:232 \n","473110size:\n","size:   473index_:\n","473index:\n","\n","473 index_:\n","358  index_:index_:\n"," 232len(inp_filenames):219110 \n","347\n","index:\n"," \n","len(inp_filenames):249  219len(inp_filenames):473\n","size:473\n","\n","\n","size: len(inp_filenames): len(tar_filenames):len(tar_filenames):   473473483483\n"," 473\n","\n","\n","index_:473\n","len(tar_filenames):\n","index_:  483110\n","\n"," len(tar_filenames):len(inp_filenames):219 483\n","\n"," len(inp_filenames): 473473\n","len(tar_filenames): 483\n","\n","len(tar_filenames):281382\n","216index:65  39382\n","\n","\n","483index:\n","size:index:\n","\n"," index: index:47365\n","index_:\n","size: 382 473 \n","281index_:\n"," size:216 \n","65len(inp_filenames):\n","\n","   len(inp_filenames):size: 473 473\n","473\n","len(tar_filenames):39\n","len(tar_filenames): 473index_:483 234\n","\n"," 483\n","\n","size:index_: 473\n"," 216\n","index_:index:296 \n","234index:\n","  size:281\n","39296\n","\n","len(inp_filenames):len(inp_filenames):size:\n","    len(inp_filenames):473473473\n","\n","index_: 473len(tar_filenames): \n"," 473410296483\n","\n","\n","\n","\n","index_:len(tar_filenames):index:len(inp_filenames):  len(tar_filenames):483410 \n","\n","483size: \n","234 473\n"," \n","len(inp_filenames):index_: 473\n","308 473len(tar_filenames):410\n","len(inp_filenames):\n","  \n","473index:483\n","\n","len(tar_filenames): 8 \n","len(tar_filenames):308index:\n"," 483size:483 \n"," \n","2654738\n","\n","size:\n"," index:index_:47322\n"," \n","308index:index_:  \n"," 228len(inp_filenames):265\n","\n"," len(inp_filenames):size:473 \n"," 35len(tar_filenames):473473\n","\n"," len(tar_filenames):index_:\n","\n","  size:48322483\n"," \n","index: 473\n","35\n","\n","len(inp_filenames):index_:47  size:265\n"," \n","473473len(inp_filenames):\n","index:\n","  len(tar_filenames):index_:47 473 \n","231483\n","72\n","35\n","\n","size: 473\n","index:index:len(tar_filenames):\n","index_:  47len(inp_filenames):  72\n","483231 \n","374\n","len(inp_filenames):473 size:\n","\n"," len(tar_filenames):473\n","size:473 index:\n","483 index_: \n","\n"," 72473len(tar_filenames):374\n","\n","\n"," index_:size:len(inp_filenames):  231473483\n","\n"," \n","len(inp_filenames):289473index_: \n","\n"," 473len(tar_filenames):index:\n"," 374 \n","len(tar_filenames):483289len(inp_filenames):\n"," 456\n","157index: \n","483 473size:456\n"," \n","473\n","len(tar_filenames):\n","index: index_:445\n","483  \n","289\n","size:index: 157\n","473 \n","len(inp_filenames):\n","445 index_:size:  \n","473456size:\n","473\n","\n","len(inp_filenames):204index_:\n","len(tar_filenames):index:   204 \n","473473 157\n","len(tar_filenames): size:483\n","\n","\n","483 len(inp_filenames):\n","450\n","index_:473  \n","473index:index_:445 \n","450 len(inp_filenames):\n","\n","20429 len(tar_filenames):\n","473len(inp_filenames): \n","size:\n","483 len(tar_filenames):473\n","  index:483473\n"," 307len(tar_filenames):\n","\n","index_:\n","29  index:\n","179 size:\n","307 450index:\n","473 483size:\n","179 \n","\n","index_:\n","473len(inp_filenames):size:\n"," index_:  47329307\n","\n","index_: len(inp_filenames):179  \n","473\n","\n","len(inp_filenames):len(inp_filenames):len(tar_filenames): 473 483\n"," len(tar_filenames):473473 \n","\n","len(tar_filenames):len(tar_filenames):  483483\n","\n","483\n","\n","68\n","467index:\n"," index:68 \n","467size:262\n"," \n","size:473index: \n"," index_:262 \n","68size:473 \n","473index_:\n"," index_:467\n"," \n","262len(inp_filenames):\n","len(inp_filenames):354 len(inp_filenames): 473 \n","473\n","473len(tar_filenames):\n","\n"," index:len(tar_filenames):len(tar_filenames):483   111\n","354136483\n","18334index:\n","\n","index: index: 136 \n","483\n","\n","\n","18334index:\n","size: size:\n","\n"," size:size: 111473 473 \n","\n","473473\n","index_:\n","size:\n","index_: index_:index_: 136   \n","3447399\n","len(inp_filenames):183\n","354 \n","len(inp_filenames):\n","\n","index: len(inp_filenames):473index_: 473\n"," len(inp_filenames): 473len(tar_filenames):9966 111\n","\n"," \n","\n","len(tar_filenames):size:\n","483len(inp_filenames): index:\n","473len(tar_filenames):327\n"," 473  \n","483 len(tar_filenames):473\n","66index:\n","483 index_:\n","\n"," 483\n","len(tar_filenames): size:  99207327\n","\n","\n","size: 473\n","473\n","index_:483len(inp_filenames): \n","\n","327 index:index_: \n"," 473len(inp_filenames):207\n","46066\n","\n","\n","size:len(inp_filenames): len(tar_filenames):   47347347313\n","\n","\n","len(tar_filenames):index:index_:   index:48313483207\n","\n","\n"," \n","57len(tar_filenames):\n","size:460len(inp_filenames): \n"," 473\n","size:483 \n"," index:len(tar_filenames):473473\n","61 \n","\n"," \n","57index_:483\n"," \n","index_:size:index:460  13 27261\n","len(inp_filenames):\n","size: \n"," len(inp_filenames):473473\n","473\n","\n","index_:\n","len(tar_filenames): index_:  57index:180\n"," 48361\n"," len(inp_filenames):272\n","\n","index:\n"," size:473 \n","473len(inp_filenames):len(tar_filenames):\n","473 index_: \n","483  180len(tar_filenames):\n","140272 473\n","\n","\n","483\n","size:index: \n","len(tar_filenames):len(inp_filenames): 140 473\n","\n","473 size:483\n","len(tar_filenames):index_: \n","  463180473\n","\n","index:index_:  463140483\n","\n","\n","\n","size:len(inp_filenames):334 209len(inp_filenames): 473\n","473 \n","index:473\n","\n","len(tar_filenames):\n"," index:index_:len(tar_filenames):334   209 \n","483167size:483\n"," \n","463\n","index:\n","473\n","len(inp_filenames):346 \n","size: 167 473\n","index_:\n","473 index:\n","size:\n","index_: 334 19len(tar_filenames):\n"," 473346 \n","483size:\n","\n","len(inp_filenames):209\n"," index_: index:432473  \n","19167len(tar_filenames):\n","\n"," len(inp_filenames):size:\n","483 473 len(inp_filenames):473\n","473\n","\n"," index_:\n"," \n","473index_:len(tar_filenames):346\n","  index:len(tar_filenames):\n","19483\n","\n"," len(inp_filenames):  432473len(inp_filenames):483\n"," \n","\n","256size:len(tar_filenames): 473\n"," 483368\n","473\n","index:\n","\n"," index_:len(tar_filenames):  index:432256483 282\n","\n","\n","\n","len(inp_filenames):size: 368 389index:473473\n"," \n","\n","\n","size:index:282index_:len(tar_filenames): \n","  389256size:483 416\n","\n"," \n","size:\n","473len(inp_filenames):473\n"," index: \n","index_:473 index_:473 368\n"," \n","416index_:444282len(tar_filenames):\n"," \n","\n","\n"," size:index:389len(inp_filenames):len(inp_filenames):  483 \n"," 473\n","\n","473444len(inp_filenames):473\n","len(tar_filenames):\n"," \n","index_: size:473len(tar_filenames):483  \n"," 416473len(tar_filenames):483\n","\n","\n"," \n","len(inp_filenames):index_:483  \n","473444\n","\n","len(tar_filenames):len(inp_filenames):  214483473\n","\n","\n","index: len(tar_filenames): 214\n","483size:469\n"," \n","452473222index:\n","\n","\n"," index:462index_:index:469 \n","  \n","index: 452214222\n","size:364462\n","size:\n","len(inp_filenames):  473\n","\n","\n","size: 473size:\n"," index:index_:  469364 473\n","\n","len(tar_filenames):473size:\n","len(inp_filenames):  473483\n","\n","473\n","index_:index_:\n","index_:   len(tar_filenames):437473 452 222\n","\n","462\n","483\n","index_:index:\n","\n","len(inp_filenames):len(inp_filenames):  len(inp_filenames): 473  473\n","364437473len(tar_filenames):\n","\n","\n","\n"," len(tar_filenames):size:483 len(tar_filenames):len(inp_filenames):   \n","483397473\n","483\n","473index_:index:\n","\n"," len(tar_filenames): 397\n"," \n","size:437 124473\n","483\n","len(inp_filenames):\n","\n","index_:index:   397473\n","124\n","len(inp_filenames): len(tar_filenames):\n"," size:473483 \n","\n","473292233104\n","\n","\n","176len(tar_filenames):\n","index:index: index_:  483 233292\n","\n","index:index: 124\n","\n","104 \n","\n","176size:size:len(inp_filenames):\n","   size:473473size:473  \n","473\n","\n","\n","473index_:index_:len(tar_filenames):\n","index_: index_:   483233292 83\n","176\n","104\n","\n","len(inp_filenames):\n"," len(inp_filenames):\n","len(inp_filenames):473 index: len(inp_filenames):\n","473  \n","83473len(tar_filenames):473len(tar_filenames): \n","\n","483 \n","483len(tar_filenames):size:len(tar_filenames):325\n","   473483\n","483\n","index:\n","\n","index_:  \n","83\n","325len(inp_filenames): 473\n","\n","len(tar_filenames): 483\n","size:31 \n","index:473245315 \n","\n","31\n","142index:index_:\n","index:\n","size:   315index: 325245304\n","\n","473\n"," \n","index:size:\n","size:142index_: \n","len(inp_filenames):   473size:31473304\n"," \n"," index_:\n","473len(inp_filenames):473 \n","\n","index_:\n"," size:len(tar_filenames):315 245 \n"," index_:\n"," 473483len(inp_filenames):len(inp_filenames):473\n","  \n","\n","142len(tar_filenames):index_:473\n"," 473\n","\n","483 len(tar_filenames):len(tar_filenames):30440len(inp_filenames):\n"," \n"," \n"," 483index:483\n","len(inp_filenames): \n","473 40\n","\n","473size:len(tar_filenames):\n","  len(tar_filenames):473483 \n","422483123index_:\n","\n","\n","\n","index: index: 40123 97\n","\n","422len(inp_filenames):\n"," index:473size: \n","\n"," 97size:len(tar_filenames):473\n","  size:\n","483473index_:\n"," \n","473\n","index_: index_:412  12397\n","\n","422\n","len(inp_filenames):len(inp_filenames):\n","  len(inp_filenames):473 \n","index:473len(tar_filenames): \n","483 473len(tar_filenames):\n","412\n"," len(tar_filenames):483\n","\n"," size:483 \n","473\n","index_: 412\n","len(inp_filenames): 473\n"," len(tar_filenames):483\n","228\n","index: 228\n","size: 473\n","index_: 228\n","len(inp_filenames): 473\n","len(tar_filenames): 483\n","237\n","index: 237\n","size: 473\n","index_: 237\n","len(inp_filenames): 473\n","len(tar_filenames): 483\n","194\n","index: 194\n","size: 473\n","index_: 194\n","len(inp_filenames): 473\n","len(tar_filenames): 483\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/60 [00:10<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-5b0c4cdf5fa0>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mrestored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_restored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLFormer-main-g/LLFormer-main/model/LLFormer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp_img)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mout_fusion_123\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_fusion_123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLFormer-main-g/LLFormer-main/model/LLFormer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLFormer-main-g/LLFormer-main/model/LLFormer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLFormer-main-g/LLFormer-main/model/LLFormer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 14.75 GiB total capacity; 14.41 GiB already allocated; 50.81 MiB free; 14.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}]}