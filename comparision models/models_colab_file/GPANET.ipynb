{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOnglWSH4fd1ceQQxTVHFo1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CtOmouB4Sjud","executionInfo":{"status":"ok","timestamp":1709039982371,"user_tz":-330,"elapsed":4,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}}},"outputs":[],"source":["from google.colab import drive\n"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mfmKi5RSrdf","executionInfo":{"status":"ok","timestamp":1709040008392,"user_tz":-330,"elapsed":26024,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"01c457d2-d4d7-45da-ba4f-b81bbdb37f94"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","# Change the current directory\n","os.chdir('/content/drive/MyDrive/GPANet-main/GPANet')"],"metadata":{"id":"1ACI2LShSwFQ","executionInfo":{"status":"ok","timestamp":1709040008392,"user_tz":-330,"elapsed":5,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-image\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABmnnQVjT95C","executionInfo":{"status":"ok","timestamp":1709040014319,"user_tz":-330,"elapsed":5931,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"182155d9-7294-4e7b-b997-d8427c56648d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.25.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.2.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Different utilities such as orthogonalization of weights, initialization of\n","loggers, etc\n","\n","Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n","\n","This program is free software: you can use, modify and/or\n","redistribute it under the terms of the GNU General Public\n","License as published by the Free Software Foundation, either\n","version 3 of the License, or (at your option) any later\n","version. You should have received a copy of this license along\n","this program. If not, see <http://www.gnu.org/licenses/>.\n","\"\"\"\n","import subprocess\n","import math\n","import logging\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n","\n","from copy import copy\n","import torch.nn.functional as F\n","\n","def weights_init_kaiming(lyr):\n","\tr\"\"\"Initializes weights of the model according to the \"He\" initialization\n","\tmethod described in \"Delving deep into rectifiers: Surpassing human-level\n","    performance on ImageNet classification\" - He, K. et al. (2015), using a\n","    normal distribution.\n","\tThis function is to be called by the torch.nn.Module.apply() method,\n","\twhich applies weights_init_kaiming() to every layer of the model.\n","\t\"\"\"\n","\tclassname = lyr.__class__.__name__\n","\tif classname.find('Conv') != -1:\n","\t\tnn.init.kaiming_normal_(lyr.weight.data, a=0, mode='fan_in')\n","\telif classname.find('Linear') != -1:\n","\t\tnn.init.kaiming_normal_(lyr.weight.data, a=0, mode='fan_in')\n","\telif classname.find('BatchNorm') != -1:\n","\t\tlyr.weight.data.normal_(mean=0, std=math.sqrt(2./9./64.)).\\\n","\t\t\tclamp_(-0.025, 0.025)\n","\t\tnn.init.constant_(lyr.bias.data, 0.0)\n","\n","def batch_psnr(img, imclean, data_range):\n","\tr\"\"\"\n","\tComputes the PSNR along the batch dimension (not pixel-wise)\n","\n","\tArgs:\n","\t\timg: a `torch.Tensor` containing the restored image\n","\t\timclean: a `torch.Tensor` containing the reference image\n","\t\tdata_range: The data range of the input image (distance between\n","\t\t\tminimum and maximum possible values). By default, this is estimated\n","\t\t\tfrom the image data-type.\n","\t\"\"\"\n","\timg_cpu = img.data.cpu().numpy().astype(np.float32)\n","\timgclean = imclean.data.cpu().numpy().astype(np.float32)\n","\tpsnr = 0\n","\tfor i in range(img_cpu.shape[0]):\n","\t\tpsnr += compare_psnr(imgclean[i, :, :, :], img_cpu[i, :, :, :], \\\n","\t\t\t\t\t   data_range=data_range)\n","\treturn psnr/img_cpu.shape[0]\n","\n","def data_augmentation(image, mode):\n","\tr\"\"\"Performs dat augmentation of the input image\n","\n","\tArgs:\n","\t\timage: a cv2 (OpenCV) image\n","\t\tmode: int. Choice of transformation to apply to the image\n","\t\t\t0 - no transformation\n","\t\t\t1 - flip up and down\n","\t\t\t2 - rotate counterwise 90 degree\n","\t\t\t3 - rotate 90 degree and flip up and down\n","\t\t\t4 - rotate 180 degree\n","\t\t\t5 - rotate 180 degree and flip\n","\t\t\t6 - rotate 270 degree\n","\t\t\t7 - rotate 270 degree and flip\n","\t\"\"\"\n","\tout = np.transpose(image, (1, 2, 0))\n","\tif mode == 0:\n","\t\t# original\n","\t\tout = out\n","\telif mode == 1:\n","\t\t# flip up and down\n","\t\tout = np.flipud(out)\n","\telif mode == 2:\n","\t\t# rotate counterwise 90 degree\n","\t\tout = np.rot90(out)\n","\telif mode == 3:\n","\t\t# rotate 90 degree and flip up and down\n","\t\tout = np.rot90(out)\n","\t\tout = np.flipud(out)\n","\telif mode == 4:\n","\t\t# rotate 180 degree\n","\t\tout = np.rot90(out, k=2)\n","\telif mode == 5:\n","\t\t# rotate 180 degree and flip\n","\t\tout = np.rot90(out, k=2)\n","\t\tout = np.flipud(out)\n","\telif mode == 6:\n","\t\t# rotate 270 degree\n","\t\tout = np.rot90(out, k=3)\n","\telif mode == 7:\n","\t\t# rotate 270 degree and flip\n","\t\tout = np.rot90(out, k=3)\n","\t\tout = np.flipud(out)\n","\telse:\n","\t\traise Exception('Invalid choice of image transformation')\n","\treturn np.transpose(out, (2, 0, 1))\n","\n","def variable_to_cv2_image(varim):\n","\tr\"\"\"Converts a torch.autograd.Variable to an OpenCV image\n","\n","\tArgs:\n","\t\tvarim: a torch.autograd.Variable\n","\t\"\"\"\n","\tnchannels = varim.size()[1]\n","\tif nchannels == 1:\n","\t\tres = (varim.data.cpu().numpy()[0, 0, :]*255.).clip(0, 255).astype(np.uint8)\n","\telif nchannels == 3:\n","\t\tres = varim.data.cpu().numpy()[0]\n","\t\tres = cv2.cvtColor(res.transpose(1, 2, 0), cv2.COLOR_RGB2BGR)\n","\t\tres = (res*255.).clip(0, 255).astype(np.uint8)\n","\telse:\n","\t\traise Exception('Number of color channels not supported')\n","\treturn res\n","\n","def get_git_revision_short_hash():\n","\tr\"\"\"Returns the current Git commit.\n","\t\"\"\"\n","\treturn subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip()\n","\n","def init_logger(argdict):\n","\tr\"\"\"Initializes a logging.Logger to save all the running parameters to a\n","\tlog file\n","\n","\tArgs:\n","\t\targdict: dictionary of parameters to be logged\n","\t\"\"\"\n","\tfrom os.path import join\n","\n","\tlogger = logging.getLogger(__name__)\n","\tlogger.setLevel(level=logging.INFO)\n","\tfh = logging.FileHandler(join(argdict.log_dir, 'log.txt'), mode='a')\n","\tformatter = logging.Formatter('%(asctime)s - %(message)s')\n","\tfh.setFormatter(formatter)\n","\tlogger.addHandler(fh)\n","\ttry:\n","\t\tlogger.info(\"Commit: {}\".format(get_git_revision_short_hash()))\n","\texcept Exception as e:\n","\t\tlogger.error(\"Couldn't get commit number: {}\".format(e))\n","\tlogger.info(\"Arguments: \")\n","\tfor k in argdict.__dict__:\n","\t\tlogger.info(\"\\t{}: {}\".format(k, argdict.__dict__[k]))\n","\n","\treturn logger\n","\n","def init_logger_ipol():\n","\tr\"\"\"Initializes a logging.Logger in order to log the results after\n","\ttesting a model\n","\n","\tArgs:\n","\t\tresult_dir: path to the folder with the denoising results\n","\t\"\"\"\n","\tlogger = logging.getLogger('testlog')\n","\tlogger.setLevel(level=logging.INFO)\n","\tfh = logging.FileHandler('out.txt', mode='w')\n","\tformatter = logging.Formatter('%(message)s')\n","\tfh.setFormatter(formatter)\n","\tlogger.addHandler(fh)\n","\n","\treturn logger\n","\n","def init_logger_test(result_dir):\n","\tr\"\"\"Initializes a logging.Logger in order to log the results after testing\n","\ta model\n","\n","\tArgs:\n","\t\tresult_dir: path to the folder with the denoising results\n","\t\"\"\"\n","\tfrom os.path import join\n","\n","\tlogger = logging.getLogger('testlog')\n","\tlogger.setLevel(level=logging.INFO)\n","\tfh = logging.FileHandler(join(result_dir, 'log.txt'), mode='a')\n","\tformatter = logging.Formatter('%(asctime)s - %(message)s')\n","\tfh.setFormatter(formatter)\n","\tlogger.addHandler(fh)\n","\n","\treturn logger\n","\n","def normalize(data):\n","\tr\"\"\"Normalizes a unit8 image to a float32 image in the range [0, 1]\n","\n","\tArgs:\n","\t\tdata: a unint8 numpy array to normalize from [0, 255] to [0, 1]\n","\t\"\"\"\n","\treturn np.float32(data/255.)\n","\n","def svd_orthogonalization(lyr):\n","\tr\"\"\"Applies regularization to the training by performing the\n","\torthogonalization technique described in the paper \"FFDNet:\tToward a fast\n","\tand flexible solution for CNN based image denoising.\" Zhang et al. (2017).\n","\tFor each Conv layer in the model, the method replaces the matrix whose columns\n","\tare the filters of the layer by new filters which are orthogonal to each other.\n","\tThis is achieved by setting the singular values of a SVD decomposition to 1.\n","\n","\tThis function is to be called by the torch.nn.Module.apply() method,\n","\twhich applies svd_orthogonalization() to every layer of the model.\n","\t\"\"\"\n","\tclassname = lyr.__class__.__name__\n","\tif classname.find('Conv') != -1:\n","\t\tweights = lyr.weight.data.clone()\n","\t\tc_out, c_in, f1, f2 = weights.size()\n","\t\tdtype = lyr.weight.data.type()\n","\n","\t\t# Reshape filters to columns\n","\t\t# From (c_out, c_in, f1, f2)  to (f1*f2*c_in, c_out)\n","\t\tweights = weights.permute(2, 3, 1, 0).contiguous().view(f1*f2*c_in, c_out)\n","\n","\t\t# Convert filter matrix to numpy array\n","\t\tweights = weights.cpu().numpy()\n","\n","\t\t# SVD decomposition and orthogonalization\n","\t\tmat_u, _, mat_vh = np.linalg.svd(weights, full_matrices=False)\n","\t\tweights = np.dot(mat_u, mat_vh)\n","\n","\t\t# As full_matrices=False we don't need to set s[:] = 1 and do mat_u*s\n","\t\tlyr.weight.data = torch.Tensor(weights).view(f1, f2, c_in, c_out).\\\n","\t\t\tpermute(3, 2, 0, 1).type(dtype)\n","\telse:\n","\t\tpass\n","\n","def remove_dataparallel_wrapper(state_dict):\n","\tr\"\"\"Converts a DataParallel model to a normal one by removing the \"module.\"\n","\twrapper in the module dictionary\n","\n","\tArgs:\n","\t\tstate_dict: a torch.nn.DataParallel state dictionary\n","\t\"\"\"\n","\tfrom collections import OrderedDict\n","\n","\tnew_state_dict = OrderedDict()\n","\tfor k, vl in state_dict.items():\n","\t\tname = k[7:] # remove 'module.' of DataParallel\n","\t\tnew_state_dict[name] = vl\n","\n","\treturn new_state_dict\n","\n","def is_rgb(im_path):\n","\tr\"\"\" Returns True if the image in im_path is an RGB image\n","\t\"\"\"\n","\tfrom skimage.io import imread\n","\trgb = False\n","\tim = imread(im_path)\n","\tif (len(im.shape) == 3):\n","\t\tif not(np.allclose(im[...,0], im[...,1]) and np.allclose(im[...,2], im[...,1])):\n","\t\t\trgb = True\n","\tprint(\"rgb: {}\".format(rgb))\n","\tprint(\"im shape: {}\".format(im.shape))\n","\treturn rgb\n","\n","\n"],"metadata":{"id":"ouIDW8YZUm7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python test_demo.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKZFyYQhT8gy","executionInfo":{"status":"ok","timestamp":1709041791671,"user_tz":-330,"elapsed":496,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"d7562f5f-24b9-4cac-f9a7-79b6f9c87c7b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["  File \"/content/drive/MyDrive/GPANet-main/GPANet/test_demo.py\", line 128\n","    img_g = img_g / 255.0 \n","                          ^\n","IndentationError: unindent does not match any outer indentation level\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Mar 21 20:48:05 2020\n","@author: Administrator\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import cv2\n","import time\n","import os\n","from model import *\n","import utils_train\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","def load_checkpoint(checkpoint_dir, IsGPU):\n","    if IsGPU == 1:\n","        model_info = torch.load(checkpoint_dir + 'checkpoint.pth.tar')\n","        net = Main()\n","        device_ids = [0]\n","        model = nn.DataParallel(net, device_ids=device_ids).cuda()\n","        model.load_state_dict(model_info['state_dict'])\n","        optimizer = torch.optim.Adam(model.parameters())\n","        optimizer.load_state_dict(model_info['optimizer'])\n","        cur_epoch = model_info['epoch']\n","    else:\n","        model_info = torch.load(checkpoint_dir + 'checkpoint.pth.tar', map_location=torch.device('cpu'))\n","        net = Main()\n","        device_ids = [0]\n","        model = nn.DataParallel(net, device_ids=device_ids)\n","        model.load_state_dict(model_info['state_dict'])\n","        optimizer = torch.optim.Adam(model.parameters())\n","        optimizer.load_state_dict(model_info['optimizer'])\n","        cur_epoch = model_info['epoch']\n","    return model, optimizer, cur_epoch\n","\n","def adjust_learning_rate(optimizer, epoch, lr_update_freq):\n","    if not epoch % lr_update_freq and epoch:\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = param_group['lr'] * 0.1\n","            print(param_group['lr'])\n","    return optimizer\n","\n","def FSobel_XY(data):\n","    data = cv2.cvtColor(chw_to_hwc(data), cv2.COLOR_BGR2GRAY)\n","    x = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 0, 1, ksize=3)\n","    absX = cv2.convertScaleAbs(x)\n","    y = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 1, 0, ksize=3)\n","    absY = cv2.convertScaleAbs(y)\n","    dst = cv2.addWeighted(absX, 0.5, absY, 0.5, 0)\n","    return dst / 255.0\n","\n","def FLap(data):\n","    data = cv2.cvtColor(chw_to_hwc(data), cv2.COLOR_BGR2GRAY)\n","    x = cv2.GaussianBlur(data, (3,3), 0)\n","    x = cv2.Laplacian(np.clip(x*255, 0, 255).astype('uint8'), cv2.CV_8U, ksize=3)\n","    Lap = cv2.convertScaleAbs(x)\n","    return Lap / 255.0\n","\n","def GFSobel_XY(data):\n","    x = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 0, 1, ksize=3)\n","    absX = cv2.convertScaleAbs(x)\n","    y = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 1, 0, ksize=3)\n","    absY = cv2.convertScaleAbs(y)\n","    dst = cv2.addWeighted(absX, 0.5, absY, 0.5, 0)\n","    return dst / 255.0\n","\n","def GFLap(data):\n","    x = cv2.GaussianBlur(data, (3,3), 0)\n","    x = cv2.Laplacian(np.clip(x*255, 0, 255).astype('uint8'), cv2.CV_8U, ksize=3)\n","    Lap = cv2.convertScaleAbs(x)\n","    return Lap / 255.0\n","\n","def hwc_to_chw(img):\n","    return np.transpose(img, axes=[2, 0, 1])\n","\n","def chw_to_hwc(img):\n","    return np.transpose(img, axes=[1, 2, 0])\n","\n","if __name__ == '__main__':\n","    checkpoint_dir = './checkpoint/'\n","    test_dir = '/content/drive/MyDrive/GPANet-main/GPANet/dataset/low/'\n","    result_dir = '/content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL'\n","    testfiles = os.listdir(test_dir)\n","    IsGPU = 1    #GPU is 1, CPU is 0\n","\n","    print('> Loading dataset ...')\n","\n","    lr_update_freq = 30\n","    model, optimizer, cur_epoch = load_checkpoint(checkpoint_dir, IsGPU)\n","\n","    if IsGPU == 1:\n","        for f in range(len(testfiles)):\n","            model.eval()\n","            with torch.no_grad():\n","                img = cv2.imread(test_dir + testfiles[f])\n","                img_g = cv2.imread(test_dir + testfiles[f], 0)\n","                img_g = img_g / 255.0\n","                h, w, c = img.shape\n","                img_ccc = img / 255.0\n","                img_h = hwc_to_chw(img_ccc)\n","                input_var = torch.from_numpy(img_h.copy()).type(torch.FloatTensor).unsqueeze(0).cuda()\n","                input_svar = torch.from_numpy(GFSobel_XY(img_g.copy())).type(torch.FloatTensor).unsqueeze(0).unsqueeze(0).cuda()\n","                input_lapvar = torch.from_numpy(GFLap(img_g.copy())).type(torch.FloatTensor).unsqueeze(0).unsqueeze(0).cuda()\n","                s = time.time()\n","                lapout, lbpout, e_out = model(input_var, input_svar, input_lapvar)\n","                e = time.time()\n","                print(input_var.shape)\n","                print(e-s)\n","                e_out = e_out.squeeze().cpu().detach().numpy()\n","                e_out = chw_to_hwc(e_out)\n","                e_out = cv2.resize(e_out, (w, h))\n","                newlap = np.zeros((e_out.shape))\n","                newlbp = np.zeros((e_out.shape))\n","                lap_out = lapout.squeeze().squeeze().cpu().detach().numpy()\n","                lap_out = cv2.resize(lap_out, (w, h))\n","                for clap in range(3):\n","                    newlap[:,:,clap] = lap_out\n","                lbp_out = lbpout.squeeze().squeeze().cpu().detach().numpy()\n","                lbp_out = cv2.resize(lbp_out, (w, h))\n","                for clbp in range(3):\n","                    newlbp[:,:,clbp] = lbp_out\n","\n","                temp = e_out\n","\n","                print(\"Saving result for:\", result_dir + '/' + testfiles[f][:-4] + '_Grad4' + '.png')\n","\n","                cv2.imwrite(result_dir + '/' + testfiles[f][:-4] + '_Grad4' + '.png', np.clip(temp * 255, 0.0, 255.0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvforatY0qbD","executionInfo":{"status":"ok","timestamp":1709042805449,"user_tz":-330,"elapsed":3994,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"804bb2fa-f9b5-48ea-ac04-b56de148c3da"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["> Loading dataset ...\n","torch.Size([1, 3, 800, 600])\n","0.0373539924621582\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/146_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.011958122253417969\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/669_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.01083517074584961\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/79_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.012266159057617188\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/22_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.01016545295715332\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/778_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010848045349121094\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/179_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010802984237670898\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/55_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010236740112304688\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/547_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.01026153564453125\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/1_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.013139724731445312\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/493_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010326385498046875\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/665_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010867595672607422\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/748_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.010529279708862305\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/780_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.017755746841430664\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/23_Grad4.png\n","torch.Size([1, 3, 800, 600])\n","0.011710882186889648\n","Saving result for: /content/drive/MyDrive/GPANet-main/GPANet/dataset/Test_result/LOL/111_Grad4.png\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","# Directory containing the images\n","input_dir = \"/content/drive/MyDrive/LOLdataset/eval15/low/\"\n","output_dir = \"/content/drive/MyDrive/GPANet-main/GPANet/dataset/low/\"\n","\n","# Ensure the output directory exists\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Iterate over all files in the input directory\n","for filename in os.listdir(input_dir):\n","    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust file extensions as needed\n","        # Open the image\n","        img_path = os.path.join(input_dir, filename)\n","        img = Image.open(img_path)\n","\n","        # Resize the image to 800x600\n","        resized_img = img.resize((600, 800))\n","\n","        # Save the resized image to the output directory\n","        output_path = os.path.join(output_dir, filename)\n","        resized_img.save(output_path)\n","\n","        print(f\"Resized {filename} successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-_CpPwTv8EI","executionInfo":{"status":"ok","timestamp":1709040697551,"user_tz":-330,"elapsed":5066,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"1c4dfd04-54de-46c0-89e1-c513a4302a85"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Resized 146.png successfully.\n","Resized 669.png successfully.\n","Resized 79.png successfully.\n","Resized 22.png successfully.\n","Resized 778.png successfully.\n","Resized 179.png successfully.\n","Resized 55.png successfully.\n","Resized 547.png successfully.\n","Resized 1.png successfully.\n","Resized 493.png successfully.\n","Resized 665.png successfully.\n","Resized 748.png successfully.\n","Resized 780.png successfully.\n","Resized 23.png successfully.\n","Resized 111.png successfully.\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","# Directory containing the images\n","input_dir = \"/content/drive/MyDrive/LOLdataset/eval15/high/\"\n","output_dir = \"/content/drive/MyDrive/GPANet-main/GPANet/Clear/\"\n","\n","# Ensure the output directory exists\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Iterate over all files in the input directory\n","for filename in os.listdir(input_dir):\n","    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust file extensions as needed\n","        # Open the image\n","        img_path = os.path.join(input_dir, filename)\n","        img = Image.open(img_path)\n","\n","        # Resize the image to 800x600\n","        resized_img = img.resize((600, 800))\n","\n","        # Save the resized image to the output directory\n","        output_path = os.path.join(output_dir, filename)\n","        resized_img.save(output_path)\n","\n","        print(f\"Resized {filename} successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZW7D4PHweEv","executionInfo":{"status":"ok","timestamp":1709041260504,"user_tz":-330,"elapsed":7863,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"5525fb76-4115-4a67-c559-3b69d50354ef"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Resized 146.png successfully.\n","Resized 669.png successfully.\n","Resized 79.png successfully.\n","Resized 22.png successfully.\n","Resized 778.png successfully.\n","Resized 179.png successfully.\n","Resized 55.png successfully.\n","Resized 1.png successfully.\n","Resized 547.png successfully.\n","Resized 493.png successfully.\n","Resized 748.png successfully.\n","Resized 780.png successfully.\n","Resized 665.png successfully.\n","Resized 23.png successfully.\n","Resized 111.png successfully.\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import cv2\n","import time\n","import os\n","from model import *\n","import utils_train\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","def load_checkpoint(checkpoint_dir, IsGPU):\n","    if IsGPU == 1:\n","        model_info = torch.load(checkpoint_dir + 'checkpoint.pth.tar')\n","        net = Main()\n","        device_ids = [0]\n","        model = nn.DataParallel(net, device_ids=device_ids).cuda()\n","        model.load_state_dict(model_info['state_dict'])\n","        optimizer = torch.optim.Adam(model.parameters())\n","        optimizer.load_state_dict(model_info['optimizer'])\n","        cur_epoch = model_info['epoch']\n","    else:\n","        model_info = torch.load(checkpoint_dir + 'checkpoint.pth.tar', map_location=torch.device('cpu'))\n","        net = Main()\n","        device_ids = [0]\n","        model = nn.DataParallel(net, device_ids=device_ids)\n","        model.load_state_dict(model_info['state_dict'])\n","        optimizer = torch.optim.Adam(model.parameters())\n","        optimizer.load_state_dict(model_info['optimizer'])\n","        cur_epoch = model_info['epoch']\n","\n","    return model, optimizer, cur_epoch\n","\n","def adjust_learning_rate(optimizer, epoch, lr_update_freq):\n","    if not epoch % lr_update_freq and epoch:\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = param_group['lr'] * 0.1\n","            print(param_group['lr'])\n","    return optimizer\n","\n","def train_psnr(train_in, train_out):\n","    psnr = utils_train.batch_psnr(train_in, train_out, 1.)\n","    return psnr\n","\n","def FSobel_XY(data):\n","    data = cv2.cvtColor(chw_to_hwc(data), cv2.COLOR_BGR2GRAY)\n","    x = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 0, 1, ksize=3)\n","    absX = cv2.convertScaleAbs(x)\n","    y = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 1, 0, ksize=3)\n","    absY = cv2.convertScaleAbs(y)\n","    dst = cv2.addWeighted(absX, 0.5, absY, 0.5, 0)\n","    return dst / 255.0\n","\n","def FLap(data):\n","    data = cv2.cvtColor(chw_to_hwc(data), cv2.COLOR_BGR2GRAY)\n","    x = cv2.GaussianBlur(data, (3, 3), 0)\n","    x = cv2.Laplacian(np.clip(x*255, 0, 255).astype('uint8'), cv2.CV_8U, ksize=3)\n","    Lap = cv2.convertScaleAbs(x)\n","    return Lap / 255.0\n","\n","def GFSobel_XY(data):\n","    x = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 0, 1, ksize=3)\n","    absX = cv2.convertScaleAbs(x)\n","    y = cv2.Sobel(np.clip(data*255, 0, 255).astype('uint8'), cv2.CV_8U, 1, 0, ksize=3)\n","    absY = cv2.convertScaleAbs(y)\n","    dst = cv2.addWeighted(absX, 0.5, absY, 0.5, 0)\n","    return dst / 255.0\n","\n","def GFLap(data):\n","    x = cv2.GaussianBlur(data, (3, 3), 0)\n","    x = cv2.Laplacian(np.clip(x*255, 0, 255).astype('uint8'), cv2.CV_8U, ksize=3)\n","    Lap = cv2.convertScaleAbs(x)\n","    return Lap / 255.0\n","\n","def hwc_to_chw(img):\n","    return np.transpose(img, axes=[2, 0, 1])\n","\n","def chw_to_hwc(img):\n","    return np.transpose(img, axes=[1, 2, 0])\n","\n","if __name__ == '__main__':\n","    checkpoint_dir = './checkpoint/'\n","    test_dir = '/content/drive/MyDrive/GPANet-main/GPANet/dataset/low/'\n","    #test_dir_1 = '/content/drive/MyDrive/LOLdataset/eval15/high/'\n","    result_dir = './Test_result'\n","    testfiles = os.listdir(test_dir)\n","    testfiles_1 = os.listdir(test_dir_1)\n","    IsGPU = 1    # GPU is 1, CPU is 0\n","\n","    print('> Loading dataset ...')\n","\n","    lr_update_freq = 30\n","    model, optimizer, cur_epoch = load_checkpoint(checkpoint_dir, IsGPU)\n","\n","    if IsGPU == 1:\n","        for f in range(len(testfiles)):\n","            model.eval()\n","            with torch.no_grad():\n","                img = cv2.imread(test_dir  +'/'+ testfiles[f])\n","                img_g = cv2.imread(test_dir + '/' + testfiles[f],0)\n","\n","                img_g = img_g / 255.0\n","                h, w, c = img.shape\n","                img_ccc = img / 255.0\n","                img_h = hwc_to_chw(img_ccc)\n","                input_var = torch.from_numpy(img_h.copy()).type(torch.FloatTensor).unsqueeze(0).cuda()\n","\n","                input_svar = torch.from_numpy(GFSobel_XY(img_g.copy())).type(torch.FloatTensor).unsqueeze(0).cuda()\n","                input_lapvar = torch.from_numpy(GFLap(img_g.copy())).type(torch.FloatTensor).unsqueeze(0).cuda()\n","\n","                s = time.time()\n","                lapout, lbpout, e_out = model(input_var, input_svar, input_lapvar)\n","                e = time.time()\n","                print(input_var.shape)\n","                print(e-s)\n","\n","                e_out = e_out.squeeze().cpu().detach().numpy()\n","                e_out = chw_to_hwc(e_out)\n","                e_out = cv2.resize(e_out, (w, h))\n","\n","                newlap = np.zeros((e_out.shape))\n","                newlbp = np.zeros((e_out.shape))\n","\n","                lap_out = lapout.squeeze().squeeze().cpu().detach().numpy()\n","                lap_out = cv2.resize(lap_out, (w, h))\n","\n","                for clap in range(3):\n","                    newlap[:, :, clap] = lap_out\n","\n","                lbp_out = lbpout.squeeze().squeeze().cpu().detach().numpy()\n","                lbp_out = cv2.resize(lbp_out, (w, h))\n","\n","                for clbp in range(3):\n","                    newlbp[:, :, clbp] = lbp_out\n","\n","                temp = e_out  # np.concatenate((img/255,e_out,newlap, newlbp), axis=1)\n","\n","                cv2.imwrite(result_dir + '/' + testfiles[f][:-4] + '_Grad4' + '.png', np.clip(temp*255, 0.0, 255.0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"Mk2flA-BW1SZ","executionInfo":{"status":"error","timestamp":1709041467106,"user_tz":-330,"elapsed":516,"user":{"displayName":"Devanand C M","userId":"10845017674034799015"}},"outputId":"52a6469f-7c3f-4ee1-f668-525ab409ce01"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["> Loading dataset ...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 4 and 3","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-1f699c472363>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mlapout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbpout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_svar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lapvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GPANet-main/GPANet/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xin, xs, xl)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"]}]}]}